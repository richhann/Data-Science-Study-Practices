{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making the split, the train_test_split function shuffles the dataset using apseudorandom number generator. If we just took the last 25% of the data as a test set,all the data points would have the label 2, as the data points are sorted by the label(see the output for iris['target'] shown earlier). Using a test set containing onlyone of the three classes would not tell us much about how well our model generalizes. This will result in situation that the training data and testing data have different distributions.So, we shuffle our data to make sure the test data contains data from all classes.To make sure that we will get the same output if we run the same function severaltimes, we provide the pseudorandom number generator with a fixed seed using therandom_state parameter. This will make the outcome deterministic, so this line willalways have the same outcome. We will always fix the random_state in this way whenusing randomized procedures.The output of the train_test_split function is X_train, X_test, y_train, andy_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,and X_test contains the remaining 25%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, random_state=142)print(\"X_train shape: {}\".format(X_train.shape))print(\"y_train shape: {}\".format(y_train.shape))print(\"X_test shape: {}\".format(X_test.shape))print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### K-Nearest Neighbours ClassifierNow we can start building the actual machine learning model. There are many classificationalgorithms in scikit-learn that we could use. Here we will use a k-nearestneighbors classifier, which is easy to understand. Building this model only consists ofstoring the training set. To make a prediction for a new data point, the algorithmfinds the point in the training set that is closest to the new point. Then it assigns thelabel of this training point to the new data point.All machine learning models in scikit-learn are implemented in their own classes,which are called Estimator classes. The k-nearest neighbors classification algorithmis implemented in the KNeighborsClassifier class in the neighbors module. Beforewe can use the model, we need to instantiate the class into an object. This is when wewill set any parameters of the model. The most important parameter of KNeighborsClassifier is the number of neighbors (i.e., \\( K \\)), which we will set to 1 for our first exploration.**Model Training**: To build the model on the training set, we call the 'fit' method of the knn object,which takes as arguments the NumPy array X_train containing the training data andthe NumPy array y_train of the corresponding training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KNN classifierfrom sklearn.neighbors import KNeighborsClassifier# Build a KNN classifier modelclf_knn = KNeighborsClassifier(n_neighbors=1)# Train the model with the training dataclf_knn.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
