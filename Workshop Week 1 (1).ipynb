
   {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP2200/COMP6200 Week 1 Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this week's practical is to get you started using Python, Jupyter Notebooks, and Git, three tools that you will use through the semester in your work.  \n",
    "\n",
    "**Python** is our language of choice in COMP2200/COMP6200. If you do not have any experience of using Python, you need to learn basic Python coding.\n",
    "\n",
    "You are looking at a **Jupyter Notebook**, it is a document that mixes text, code and the output of the code. A lot of your work will be creating notebooks like this to present your analysis.  \n",
    "\n",
    "**Git** is a distributed version control system (DVCS), you will use it to keep track of your work and ensure that you have a backup copy of what you are doing. You should have checked this notebook out of **Github** using Git. Your task this week is to complete some programming work in this worksheet and commit your changes to your own Bitbucket repository."
   ]
  }, {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "from sklearn.model_selection import train_test_split"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "Before making the split, the train_test_split function shuffles the dataset using a",
            "pseudorandom number generator. If we just took the last 25% of the data as a test set,",
            "all the data points would have the label 2, as the data points are sorted by the label",
            "(see the output for iris['target'] shown earlier). Using a test set containing only",
            "one of the three classes would not tell us much about how well our model generalizes. This will result in situation that the training data and testing data have different distributions.",
            "So, we shuffle our data to make sure the test data contains data from all classes.",
            "",
            "To make sure that we will get the same output if we run the same function several",
            "times, we provide the pseudorandom number generator with a fixed seed using the",
            "random_state parameter. This will make the outcome deterministic, so this line will",
            "always have the same outcome. We will always fix the random_state in this way when",
            "using randomized procedures.",
            "",
            "The output of the train_test_split function is X_train, X_test, y_train, and",
            "y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,",
            "and X_test contains the remaining 25%:"
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, random_state=142)",
            "",
            "print(\"X_train shape: {}\".format(X_train.shape))",
            "print(\"y_train shape: {}\".format(y_train.shape))",
            "print(\"X_test shape: {}\".format(X_test.shape))",
            "print(\"y_test shape: {}\".format(y_test.shape))"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "* ### K-Nearest Neighbours Classifier",
            "",
            "Now we can start building the actual machine learning model. There are many classification",
            "algorithms in scikit-learn that we could use. Here we will use a k-nearest",
            "neighbors classifier, which is easy to understand. Building this model only consists of",
            "storing the training set. To make a prediction for a new data point, the algorithm",
            "finds the point in the training set that is closest to the new point. Then it assigns the",
            "label of this training point to the new data point.",
            "",
            "All machine learning models in scikit-learn are implemented in their own classes,",
            "which are called Estimator classes. The k-nearest neighbors classification algorithm",
            "is implemented in the KNeighborsClassifier class in the neighbors module. Before",
            "we can use the model, we need to instantiate the class into an object. This is when we",
            "will set any parameters of the model. The most important parameter of KNeighbors",
            "Classifier is the number of neighbors (i.e., \( K \)), which we will set to 1 for our first exploration.",
            "",
            "**Model Training**: To build the model on the training set, we call the 'fit' method of the knn object,",
            "which takes as arguments the NumPy array X_train containing the training data and",
            "the NumPy array y_train of the corresponding training labels."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Import the KNN classifier",
            "from sklearn.neighbors import KNeighborsClassifier",
            "",
            "# Build a KNN classifier model",
            "clf_knn = KNeighborsClassifier(n_neighbors=1)",
            "",
            "# Train the model with the training data",
            "clf_knn.fit(X_train, y_train)"
        ]
    }
]

formatted_cells

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": ["from sklearn.model_selection import train_test_split"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["Before making the split, the train_test_split function shuffles the dataset using a",
              "pseudorandom number generator. If we just took the last 25% of the data as a test set,",
              "all the data points would have the label 2, as the data points are sorted by the label",
              "(see the output for iris['target'] shown earlier). Using a test set containing only",
              "one of the three classes would not tell us much about how well our model generalizes. This will result in situation that the training data and testing data have different distributions.",
              "So, we shuffle our data to make sure the test data contains data from all classes.",
              "",
              "To make sure that we will get the same output if we run the same function several",
              "times, we provide the pseudorandom number generator with a fixed seed using the",
              "random_state parameter. This will make the outcome deterministic, so this line will",
              "always have the same outcome. We will always fix the random_state in this way when",
              "using randomized procedures.",
              "",
              "The output of the train_test_split function is X_train, X_test, y_train, and",
              "y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,",
              "and X_test contains the remaining 25%:"]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": ["X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, random_state=142)",
              "",
              "print(\"X_train shape: {}\".format(X_train.shape))",
              "print(\"y_train shape: {}\".format(y_train.shape))",
              "print(\"X_test shape: {}\".format(X_test.shape))",
              "print(\"y_test shape: {}\".format(y_test.shape))"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["* ### K-Nearest Neighbours Classifier",
              "",
              "Now we can start building the actual machine learning model. There are many classification",
              "algorithms in scikit-learn that we could use. Here we will use a k-nearest",
              "neighbors classifier, which is easy to understand. Building this model only consists of",
              "storing the training set. To make a prediction for a new data point, the algorithm",
              "finds the point in the training set that is closest to the new point. Then it assigns the",
              "label of this training point to the new data point.",
              "",
              "All machine learning models in scikit-learn are implemented in their own classes,",
              "which are called Estimator classes. The k-nearest neighbors classification algorithm",
              "is implemented in the KNeighborsClassifier class in the neighbors module. Before",
              "we can use the model, we need to instantiate the class into an object. This is when we",
              "will set any parameters of the model. The most important parameter of KNeighbors",
              "Classifier is the number of neighbors (i.e., \( K \)), which we will set to 1 for our first exploration.",
              "",
              "**Model Training**: To build the model on the training set, we call the 'fit' method of the knn object,",
              "which takes as arguments the NumPy array X_train containing the training data and",
              "the NumPy array y_train of the corresponding training labels."]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": ["# Import the KNN classifier",
              "from sklearn.neighbors import KNeighborsClassifier",
              "",
              "# Build a KNN classifier model",
              "clf_knn = KNeighborsClassifier(n_neighbors=1)",
              "",
              "# Train the model with the training data",
              "clf_knn.fit(X_train, y_train)"]
  },{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": ["from sklearn.model_selection import train_test_split"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["Before making the split, the train_test_split function shuffles the dataset using a",
              "pseudorandom number generator. If we just took the last 25% of the data as a test set,",
              "all the data points would have the label 2, as the data points are sorted by the label",
              "(see the output for iris['target'] shown earlier). Using a test set containing only",
              "one of the three classes would not tell us much about how well our model generalizes. This will result in situation that the training data and testing data have different distributions.",
              "So, we shuffle our data to make sure the test data contains data from all classes.",
              "",
              "To make sure that we will get the same output if we run the same function several",
              "times, we provide the pseudorandom number generator with a fixed seed using the",
              "random_state parameter. This will make the outcome deterministic, so this line will",
              "always have the same outcome. We will always fix the random_state in this way when",
              "using randomized procedures.",
              "",
              "The output of the train_test_split function is X_train, X_test, y_train, and",
              "y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,",
              "and X_test contains the remaining 25%:"]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": ["X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, random_state=142)",
              "",
              "print(\"X_train shape: {}\".format(X_train.shape))",
              "print(\"y_train shape: {}\".format(y_train.shape))",
              "print(\"X_test shape: {}\".format(X_test.shape))",
              "print(\"y_test shape: {}\".format(y_test.shape))"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["* ### K-Nearest Neighbours Classifier",
              "",
              "Now we can start building the actual machine learning model. There are many classification",
              "algorithms in scikit-learn that we could use. Here we will use a k-nearest",
              "neighbors classifier, which is easy to understand. Building this model only consists of",
              "storing the training set. To make a prediction for a new data point, the algorithm",
              "finds the point in the training set that is closest to the new point. Then it assigns the",
              "label of this training point to the new data point.",
              "",
              "All machine learning models in scikit-learn are implemented in their own classes,",
              "which are called Estimator classes. The k-nearest neighbors classification algorithm",
              "is implemented in the KNeighborsClassifier class in the neighbors module. Before",
              "we can use the model, we need to instantiate the class into an object. This is when we",
              "will set any parameters of the model. The most important parameter of KNeighbors",
              "Classifier is the number of neighbors (i.e., \( K \)), which we will set to 1 for our first exploration.",
              "",
              "**Model Training**: To build the model on the training set, we call the 'fit' method of the knn object,",
              "which takes as arguments the NumPy array X_train containing the training data and",
              "the NumPy array y_train of the corresponding training labels."]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": ["# Import the KNN classifier",
              "from sklearn.neighbors import KNeighborsClassifier",
              "",
              "# Build a KNN classifier model",
              "clf_knn = KNeighborsClassifier(n_neighbors=1)",
              "",
              "# Train the model with the training data",
              "clf_knn.fit(X_train, y_train)"]
  }
 ],
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


 


